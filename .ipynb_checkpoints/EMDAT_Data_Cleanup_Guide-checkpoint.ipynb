{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71347b1c",
   "metadata": {},
   "source": [
    "# EM-DAT Data Cleanup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858502c6",
   "metadata": {},
   "source": [
    "CSCI 4622 | Introduction to Machine Learning\n",
    "Team Disaster | \n",
    "\n",
    "This notebook is resposible for the cleanup process for the EM-DAT database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4082f63",
   "metadata": {},
   "source": [
    "## Step 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'your_emdat_file.csv'  # Replace with the actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3e868",
   "metadata": {},
   "source": [
    "## Step 2: Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe69594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data information\n",
    "data.info()\n",
    "\n",
    "# Display basic statistics\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a48533",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febdc617",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168330e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b44150",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f'Duplicates: {duplicates}')\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "print(f'Data shape after removing duplicates: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63969e6c",
   "metadata": {},
   "source": [
    "### Correcting Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert all date columns to a standard format\n",
    "date_columns = ['Start_Date', 'End_Date']  # Replace with actual date columns\n",
    "for col in date_columns:\n",
    "    data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "\n",
    "# Check data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cee40",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a763bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and examine potential outliers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting a boxplot to identify outliers in key numeric columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=data.select_dtypes(include='number'))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b4dd6",
   "metadata": {},
   "source": [
    "## Step 4: Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b862694",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34234cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate disaster duration in days\n",
    "data['Disaster_Duration'] = (data['End_Date'] - data['Start_Date']).dt.days\n",
    "data[['Start_Date', 'End_Date', 'Disaster_Duration']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13430cd9",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical variables\n",
    "data = pd.get_dummies(data, columns=['Disaster_Type', 'Region'], drop_first=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db4106",
   "metadata": {},
   "source": [
    "### Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_columns = data.select_dtypes(include='number').columns\n",
    "data[num_columns] = scaler.fit_transform(data[num_columns])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf63770",
   "metadata": {},
   "source": [
    "## Step 5: Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351725c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate with additional datasets (example placeholder)\n",
    "# additional_data = pd.read_csv('additional_data.csv')\n",
    "# data = data.merge(additional_data, on='Common_Column')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac86063",
   "metadata": {},
   "source": [
    "## Step 6: Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('Target_Variable', axis=1)  # Replace with your target variable\n",
    "y = data['Target_Variable']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Training set size: {X_train.shape}')\n",
    "print(f'Test set size: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de801dd0",
   "metadata": {},
   "source": [
    "## Step 7: Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis (EDA) to ensure data readiness\n",
    "sns.pairplot(data.sample(100))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
